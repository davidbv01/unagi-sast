import OpenAI from 'openai';    
import { zodTextFormat } from 'openai/helpers/zod';
import { VulnerabilityAnalysis, VulnerabilityAnalysisSchema, VerificationRequest } from '../types';

/**
 * Verifies vulnerabilities using an LLM (OpenAI) and structured prompts.
 */
export class VulnerabilityVerifier {
  private readonly openai: OpenAI;
  private readonly model: string;

  /**
   * Creates a new VulnerabilityVerifier instance.
   * @param apiKey The OpenAI API key.
   * @param model The LLM model to use (default: gpt-4o-mini-2024-07-18).
   */
  constructor(apiKey: string, model: string = 'gpt-4o-mini-2024-07-18') {
    this.openai = new OpenAI({
      apiKey: apiKey,
    });
    this.model = model;
  }

  /**
   * Verifies a vulnerability using the LLM and a structured prompt.
   * @param request The verification request.
   * @returns The vulnerability analysis result.
   */
  public async verifyVulnerability(request: VerificationRequest): Promise<VulnerabilityAnalysis> {
    const prompt = this.buildPrompt(
      request.codeExtraction, 
      request.initialVulnerabilityAssessment,
      request.context
    );
    console.log('[VulnerabilityVerifier] Generated prompt:', prompt);
    
    const response = await this.openai.responses.parse({
      model: this.model,
      input: [
        { role: 'system', content: this.getSystemPrompt() },
        { role: 'user', content: prompt },
      ],
      text: {
        format: zodTextFormat(VulnerabilityAnalysisSchema, 'vulnerability'),
      },
    });
    if (!response.output_parsed) {
      throw new Error('LLM did not return a valid vulnerability analysis result.');
    }
    return response.output_parsed;
  }

  /**
   * Verifies multiple vulnerabilities using the LLM with shared context.
   * @param vulnerabilities Array of vulnerabilities to verify.
   * @param codeExtraction The code extraction string containing context for all vulnerabilities.
   * @param context The analysis context.
   * @returns Array of vulnerability analysis results.
   */
  public async verifyMultipleVulnerabilities(
    vulnerabilities: Array<{
      type: string;
      severity: string;
      message: string;
      description?: string;
      line?: number;
      id: string;
    }>,
    codeExtraction: string,
    context?: {
      language: string;
      framework?: string;
      additionalInfo?: string;
    }
  ): Promise<VulnerabilityAnalysis[]> {
    console.log('[VulnerabilityVerifier] Processing multiple vulnerabilities with shared context');
    
    // For now, we'll verify each vulnerability individually but with shared context
    // This could be enhanced to use a single LLM call with multiple outputs
    const results: VulnerabilityAnalysis[] = [];
    
    for (const vuln of vulnerabilities) {
      const individualPrompt = this.buildPrompt(
        codeExtraction,
        vuln,
        context
      );
      
      console.log(`[VulnerabilityVerifier] Verifying vulnerability ${vuln.id}: ${vuln.type}`);
      
      const response = await this.openai.responses.parse({
        model: this.model,
        input: [
          { role: 'system', content: this.getSystemPrompt() },
          { role: 'user', content: individualPrompt },
        ],
        text: {
          format: zodTextFormat(VulnerabilityAnalysisSchema, 'vulnerability'),
        },
      });
      
      if (!response.output_parsed) {
        throw new Error(`LLM did not return a valid vulnerability analysis result for vulnerability ${vuln.id}.`);
      }
      
      results.push(response.output_parsed);
    }
    
    return results;
  }

  /**
   * Builds the prompt for the LLM using the code extraction from CodeExtractor.
   * @param codeExtraction The formatted code extraction string from CodeExtractor.
   * @param vulnerability The vulnerability assessment details.
   * @param context The analysis context.
   * @returns The complete prompt string.
   */
  private buildPrompt(
    codeExtraction: string,
    vulnerability: {
      type: string;
      severity: string;
      message: string;
      description?: string;
      line?: number;
      id?: string;
    },
    context?: {
      language: string;
      framework?: string;
      additionalInfo?: string;
    }
  ): string {
    let prompt = `# Vulnerability Verification Request\n\n`;
    
    // Add vulnerability details
    prompt += `## Detected Vulnerability\n`;
    prompt += `- Type: ${vulnerability.type}\n`;
    prompt += `- Severity: ${vulnerability.severity}\n`;
    prompt += `- Message: ${vulnerability.message}\n`;
    if (vulnerability.description) {
      prompt += `- Description: ${vulnerability.description}\n`;
    }
    if (vulnerability.line) {
      prompt += `- Line: ${vulnerability.line}\n`;
    }
    if (vulnerability.id) {
      prompt += `- ID: ${vulnerability.id}\n`;
    }
    prompt += `\n`;
    
    // Add context if available
    if (context) {
      prompt += `## Analysis Context\n`;
      if (context.language) {
        prompt += `- Language: ${context.language}\n`;
      }
      if (context.framework) {
        prompt += `- Framework: ${context.framework}\n`;
      }
      if (context.additionalInfo) {
        prompt += `- Additional Info: ${context.additionalInfo}\n`;
      }
      prompt += `\n`;
    }
    
    // Add the formatted code extraction from CodeExtractor
    prompt += `## Code Analysis\n`;
    prompt += codeExtraction;
    prompt += `\n\n`;
    
    // Add analysis instructions
    prompt += `## Analysis Instructions\n`;
    prompt += `Analyze this vulnerability in the provided code context. Assess:\n`;
    prompt += `1. Is this a true vulnerability or a false positive?\n`;
    prompt += `2. If vulnerable, what is the exploitability and real-world impact?\n`;
    prompt += `3. Are there any sanitization measures that mitigate the risk?\n`;
    prompt += `4. What specific remediation steps should be taken?\n`;
    prompt += `\nProvide your analysis as a structured response with confidence score (0.0-1.0).`;
    
    return prompt;
  }

  /**
   * Returns the system prompt for the LLM.
   * @returns The system prompt string.
   */
  private getSystemPrompt(): string {
    return `You are a senior security researcher. Analyze the provided code for real security vulnerabilities. Assess exploitability, real-world impact, and sanitization effectiveness. Flag false positives with explanations. Give clear, actionable security recommendations. Your answer must be a valid JSON object matching the specified schema exactly.`;
  }
} 